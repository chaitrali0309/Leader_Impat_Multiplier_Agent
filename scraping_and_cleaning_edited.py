# -*- coding: utf-8 -*-
"""Scraping_and_cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_NYNNukNEY7HgW7B24A8GsPv7ckVF_Ku
"""

!pip -q install requests beautifulsoup4 lxml pandas

import re
import json
import requests
import pandas as pd
from bs4 import BeautifulSoup

PROFILE_URL = "https://prsindia.org/mptrack/17-lok-sabha/shashi-tharoor"
HEADERS = {"User-Agent": "Mozilla/5.0"}

resp = requests.get(PROFILE_URL, headers=HEADERS, timeout=30)
resp.raise_for_status()

html = resp.text
soup = BeautifulSoup(html, "html.parser")

# -------------------------
# 1) BASIC PROFILE (key:value area)
# -------------------------
def clean_text(x):
    return re.sub(r"\s+", " ", x).strip() if x else None

profile = {
    "source": "PRS India",
    "section": "basic_profile",
    "url": PROFILE_URL,
    "name": clean_text(soup.find("h1").get_text()) if soup.find("h1") else None
}

# PRS shows fields like "State :", "Constituency :", etc.
labels = ["State", "Constituency", "Party", "Nature of membership", "Start of Term", "End of Term", "No. of Term"]
for lab in labels:
    # find the exact label text like "State :"
    label_node = soup.find(string=re.compile(rf"^{lab}\s*:$"))
    if label_node:
        # value is usually in the next tag(s)
        val_tag = label_node.find_next()
        # sometimes the next is ":" container, so move until we get real text
        value = clean_text(val_tag.get_text(" ", strip=True)) if val_tag else None
        profile[lab.lower().replace(" ", "_")] = value

# Personal Profile (Age, Gender, Education) shown similarly
personal_labels = ["Age", "Gender", "Education"]
for lab in personal_labels:
    label_node = soup.find(string=re.compile(rf"^{lab}\s*:$"))
    if label_node:
        val_tag = label_node.find_next()
        profile[lab.lower()] = clean_text(val_tag.get_text(" ", strip=True)) if val_tag else None

# -------------------------
# 2) TABLES: Attendance / Debates / Questions / Pvt Bills
#    Use pandas.read_html (reliable on PRS)
# -------------------------
tables = pd.read_html(html)

# Helper to find a table by checking its columns
def find_table_by_columns(tables, required_cols):
    req = set(required_cols)
    for df in tables:
        cols = set([str(c).strip() for c in df.columns])
        if req.issubset(cols):
            return df
    return None

attendance_df = find_table_by_columns(tables, ["Session", "Attendance"])
debates_df = find_table_by_columns(tables, ["Date", "Debate title/Bill name", "Debate Type"])
questions_df = find_table_by_columns(tables, ["Date", "Title", "Type", "Ministry or Category"])

# Pvt member bills table structure can vary, so we try a few patterns:
pvt_bills_df = (
    find_table_by_columns(tables, ["Bill", "Status"]) or
    find_table_by_columns(tables, ["Title", "Status"]) or
    find_table_by_columns(tables, ["Bill name", "Status"]) or
    None
)

# -------------------------
# 3) Attach LINKS for debates/questions/bills titles (they are sansad.in)
# -------------------------
def extract_title_links(section_start_text):
    """
    Returns a list of (title_text, href) in the section that appears after a marker text.
    We use a loose heuristic: collect anchor tags until the next big heading.
    """
    marker = soup.find(string=re.compile(section_start_text))
    if not marker:
        return []
    anchors = []
    # go forward in the document and collect anchors
    for a in marker.parent.find_all_next("a", href=True):
        txt = clean_text(a.get_text(" ", strip=True))
        href = a["href"]
        # stop when we reach another major section (heuristic)
        if a.find_parent(["h2", "h3"]) and "Questions details" in a.get_text():
            pass
        if txt and ("sansad.in" in href):
            anchors.append((txt, href))
    return anchors

# Better: directly grab anchors from the Questions table area
def map_links_from_table_heading(heading_regex):
    heading = soup.find(string=re.compile(heading_regex))
    if not heading:
        return {}
    link_map = {}
    # find anchors after heading; titles are anchors to sansad.in
    for a in heading.find_all_next("a", href=True):
        txt = clean_text(a.get_text(" ", strip=True))
        href = a["href"]
        if txt and "sansad.in" in href:
            link_map[txt] = href
        # stop when we hit next major "##" heading (h2) after some distance
        if a.find_parent("h2"):
            break
    return link_map

questions_link_map = map_links_from_table_heading(r"Questions details of")
debates_link_map   = map_links_from_table_heading(r"Debates \(Participated in")
# Pvt bills link map may or may not exist; we try a broad map from all sansad links on page
all_sansad_links = {clean_text(a.get_text(" ", strip=True)): a["href"]
                   for a in soup.find_all("a", href=True) if "sansad.in" in a["href"]}

# -------------------------
# 4) Convert to JSON-safe records
# -------------------------
def df_to_records(df, link_map=None, title_col=None):
    if df is None:
        return []
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]

    # attach url if possible
    if link_map and title_col and title_col in df.columns:
        df["url"] = df[title_col].map(link_map).fillna("")
    return df.to_dict(orient="records")

attendance_data = df_to_records(attendance_df)
debates_data = df_to_records(debates_df, debates_link_map, "Debate title/Bill name")
questions_data = df_to_records(questions_df, questions_link_map, "Title")

# Pvt bills: if we couldn’t detect table reliably, we’ll keep empty for now (not “wrong”, just not found)
pvt_bills_data = df_to_records(pvt_bills_df)

# -------------------------
# 5) Save JSON files
# -------------------------
with open("prs_tharoor_basic_profile.json", "w", encoding="utf-8") as f:
    json.dump(profile, f, indent=2, ensure_ascii=False)

with open("prs_tharoor_attendance.json", "w", encoding="utf-8") as f:
    json.dump(attendance_data, f, indent=2, ensure_ascii=False)

with open("prs_tharoor_debates.json", "w", encoding="utf-8") as f:
    json.dump(debates_data, f, indent=2, ensure_ascii=False)

with open("prs_tharoor_questions.json", "w", encoding="utf-8") as f:
    json.dump(questions_data, f, indent=2, ensure_ascii=False)

with open("prs_tharoor_private_member_bills.json", "w", encoding="utf-8") as f:
    json.dump(pvt_bills_data, f, indent=2, ensure_ascii=False)

print("✅ Done.")
print("Profile:", bool(profile.get("name")))
print("Attendance rows:", len(attendance_data))
print("Debates rows:", len(debates_data))
print("Questions rows:", len(questions_data))
print("Pvt Bills rows:", len(pvt_bills_data))

from google.colab import files

files.download("prs_tharoor_basic_profile.json")
files.download("prs_tharoor_attendance.json")
files.download("prs_tharoor_debates.json")
files.download("prs_tharoor_questions.json")
files.download("prs_tharoor_private_member_bills.json")

# =========================
# FIX: PRIVATE MEMBER BILLS SCRAPER
# =========================

pvt_bills_data = []

# Find section heading
heading = soup.find(string=lambda text: text and "Private Member Bills" in text)

if heading:
    # find all links after this heading
    for a in heading.find_all_next("a", href=True):
        title = a.get_text(strip=True)
        href = a["href"]

        # Stop if we reach next major section
        if "Questions details" in title:
            break

        if title and "sansad.in" in href:
            bill_entry = {
                "source": "PRS India",
                "section": "private_member_bill",
                "bill_name": title,
                "url": href
            }
            pvt_bills_data.append(bill_entry)

print("Private Bills Found:", len(pvt_bills_data))

# Save again
with open("prs_tharoor_private_member_bills.json", "w", encoding="utf-8") as f:
    json.dump(pvt_bills_data, f, indent=2, ensure_ascii=False)

print("✅ Private bills saved.")

!pip -q install pdfplumber tqdm

import requests
import pdfplumber
import json
import io
import time
from tqdm import tqdm
import re

HEADERS = {"User-Agent": "Mozilla/5.0"}

def clean_text(text):
    if not text:
        return None
    return re.sub(r"\s+", " ", text).strip()

with open("prs_tharoor_questions.json", "r", encoding="utf-8") as f:
    questions = json.load(f)

enriched_questions = []

for q in tqdm(questions):
    url = q.get("url")

    # FIXED CONDITION
    if not url or ".pdf" not in url.lower():
        continue

    enriched_entry = q.copy()

    try:
        response = requests.get(url, headers=HEADERS)
        pdf_file = io.BytesIO(response.content)

        full_text = ""

        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    full_text += text + "\n"

        full_text = clean_text(full_text)
        enriched_entry["full_page_text"] = full_text

        # Split Question / Answer
        if "ANSWER" in full_text.upper():
            parts = re.split(r"ANSWER", full_text, maxsplit=1, flags=re.IGNORECASE)
            enriched_entry["question_text"] = clean_text(parts[0])
            enriched_entry["answer_text"] = clean_text(parts[1])
            enriched_entry["answered"] = True
        else:
            enriched_entry["question_text"] = full_text
            enriched_entry["answer_text"] = None
            enriched_entry["answered"] = False

    except Exception as e:
        enriched_entry["full_page_text"] = None
        enriched_entry["question_text"] = None
        enriched_entry["answer_text"] = None
        enriched_entry["answered"] = False

    enriched_questions.append(enriched_entry)

    time.sleep(0.7)  # safer delay

with open("prs_tharoor_questions_enriched.json", "w", encoding="utf-8") as f:
    json.dump(enriched_questions, f, indent=2, ensure_ascii=False)

print("✅ PDF Enrichment Complete.")
print("Total enriched:", len(enriched_questions))

!pip -q install tqdm

import json
import re
from tqdm import tqdm

def clean_spacing(text):
    if not text:
        return None
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def remove_headers(text):
    # Remove common header patterns
    patterns = [
        r"GOVERNMENT OF INDIA.*?QUESTION NO\.\s*\d+",
        r"LOK SABHA.*?QUESTION NO\.\s*\d+",
        r"UNSTARRED QUESTION.*?",
        r"STARRED QUESTION.*?"
    ]
    for p in patterns:
        text = re.sub(p, "", text, flags=re.IGNORECASE)
    return text

def split_question_answer(text):
    # Add an explicit check for None input
    if text is None:
        return None, None
    # Split at ANSWER keyword
    parts = re.split(r"\bANSWER\b", text, maxsplit=1, flags=re.IGNORECASE)
    if len(parts) == 2:
        return parts[0], parts[1]
    return text, None

def remove_annexure(text):
    # Remove annexure tables
    text = re.split(r"\bANNEXURE\b", text, flags=re.IGNORECASE)[0]
    return text

# Load enriched data
with open("prs_tharoor_questions_enriched.json", "r", encoding="utf-8") as f:
    questions = json.load(f)

cleaned_questions = []

for q in tqdm(questions):
    full_text = q.get("full_page_text")

    if not full_text:
        # If full_page_text is None or empty, create a cleaned entry with None for question/answer bodies
        cleaned_entry = {
            "type": "parliament_question",
            "politician": "Shashi Tharoor",
            "date": q.get("Date"),
            "title": q.get("Title"),
            "ministry": q.get("Ministry or Category"),
            "question_body": None,
            "answer_body": None,
            "status": "Not Answered",
            "source_url": q.get("url")
        }
        cleaned_questions.append(cleaned_entry)
        continue # Skip further processing for this entry

    full_text = remove_headers(full_text)
    full_text = remove_annexure(full_text)
    full_text = clean_spacing(full_text)

    question_body, answer_body = split_question_answer(full_text)

    cleaned_entry = {
        "type": "parliament_question",
        "politician": "Shashi Tharoor",
        "date": q.get("Date"),
        "title": q.get("Title"),
        "ministry": q.get("Ministry or Category"),
        "question_body": clean_spacing(question_body),
        "answer_body": clean_spacing(answer_body),
        "status": "Answered" if answer_body else "Not Answered",
        "source_url": q.get("url")
    }

    cleaned_questions.append(cleaned_entry)

with open("prs_tharoor_questions_cleaned.json", "w", encoding="utf-8") as f:
    json.dump(cleaned_questions, f, indent=2, ensure_ascii=False)

print("✅ Cleaning complete")
print("Total cleaned:", len(cleaned_questions))

import json
import re
from tqdm import tqdm

def clean_spacing(text):
    if not text:
        return None
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def extract_question_body(text):
    if not text:
        return None
    match = re.search(r"(Will the Minister.*)", text, re.IGNORECASE)
    if match:
        return match.group(1)
    return text

def clean_answer(text):
    if not text:
        return None
    # Remove minister name header
    text = re.sub(r"THE MINISTER.*?\)", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\*{3,}", "", text)
    return text.strip()

with open("prs_tharoor_questions_cleaned.json", "r", encoding="utf-8") as f:
    questions = json.load(f)

final_questions = []

for q in tqdm(questions):

    question_body = clean_spacing(q.get("question_body"))
    answer_body = clean_spacing(q.get("answer_body"))

    question_body = extract_question_body(question_body)
    answer_body = clean_answer(answer_body)

    final_entry = q.copy()
    final_entry["question_body"] = question_body
    final_entry["answer_body"] = answer_body

    final_questions.append(final_entry)

with open("prs_tharoor_questions_final.json", "w", encoding="utf-8") as f:
    json.dump(final_questions, f, indent=2, ensure_ascii=False)

print("✅ Final cleaning complete.")
print("Total:", len(final_questions))

from google.colab import files
files.download("prs_tharoor_questions_final.json")